\documentclass[10pt,twocolumn,letterpaper]{article}
\def\code#1{\texttt{#1}}
\usepackage{natbib}
%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{float}
%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
%% More packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[R]{\normalfont \normalsize \textsc{Ms HPC-AI - Big Data Project}}

\fancypagestyle{firstpage}
{
    \fancyhead[L]{ \includegraphics[width=4cm]{fig/Logo_Mines_ParisTech.svg.png}}  
    \fancyhead[R]{\normalfont \normalsize \textsc{Mines ParisTech - Ms HPC-AI}}  
}

\bibliographystyle{unsrt}
%% Title
\title{
		\huge Data Visualitation Project :\\ Ray Tracing with VTK \\
}
\usepackage{authblk}
\author{Aymeric MILLAN \& Arthur VIENS}
\affil{Lecture given by Julien Wintz}
\date{11th of March 2022}
\begin{document}

\setlength\headheight{26pt}
\lhead{\includegraphics[width=4cm]{fig/Logo_Mines_ParisTech.svg.png}}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ABSTRACT
\thispagestyle{firstpage}
\begin{abstract}
This documents contains the presentation of our work on implementing a
RayTracing algorithm using the VTK library and Python's bindings.

First, we are going to do a brief sate of the art.

Learning VTK's logic and syntax was a difficulty

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RECUPERER LE CODE
\section{Running the code}
The source code is available on my GitHub branch, under the "Project" folder.
Only the python files are the ones with the final code, not the jupyter notebook
files.

In order to run the code, you will need to have an Hadoop environment correctly
set with the right python modules installed. Also, you might need to change the
path of the data folder in order to run the ML codes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SPARK
\section{Implementation logic}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%IRIS
    \subsection{Spark ML : Iris Classification}

    In this part, we want to use the abstraction of Spark to compute machine
learning algorithms on distributed files without having to manage the
parallelism. I used three different calssifiers : 
\href{https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier}{Multilayer Perceptron},
\href{https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier}{Random Forest}
and \href{https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier}{Decision Trees}.
The image following is the number of prediction for each model, and their
scores.

\begin{figure}[H]
    \centering
    \caption{Models' scores and predictions}
    \includegraphics[width=0.45\textwidth]{fig/scores_models_spark.png}
\end{figure}

As we can see, the Random Forest Classifier seems to have the best score with
this dataset. But it is probably due to overfitting since the dataset is quite
small (150 samples).

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%PARALLELISATION
    \subsection{Spark : parallelisation of the image processing algorithm
    "MedianFilter"}

    For this part, the main challenge was to truncate the image buffer in order
to compute local median filter algorithms. To not reinvent the wheel, and to use
more librairies to this project, I used the 
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.median_filter.html}
{median\_filter} function from scipy's "ndimage" module. The true challenge of
this part was to correctly slice and concatenate the image buffer. Moreover, I
had to dive into the documentations of numpy, scipy and pyspark to know the
methods I needed. It was a lot of internet search, but I finally made it in a
easy and quickly understandable way.

\begin{figure}[H]
      \centering
      \caption{Lena before and after MedianFilter}
    \includegraphics[width=0.45\textwidth]{fig/lena_before_after.png}
\end{figure}

The algorithm splits the image in a certain number of blocks. Each process
computes locally a median filter, and we concatenate the arrays we got from each
process to rebuild the picture.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% DASK
\section{Dask}

    We are now going to do similar works that we have done with Spark, but using
the \textbf{Dask} framework. This framework is younger than Spark. Which means,
it is less mature, so we won't have as much as functionnalities and robustness,
but it is much more dynamic and user-friendly. For example, this is the output
when reading an image with dask.

    \begin{figure}[H]
          \centering
          \caption{Dask output interface}
        \includegraphics[width=0.45\textwidth]{fig/dask_interface.png}
    \end{figure}

The framework gives us, in a pretty way, the dimension of our array, the size of
it, its type, and other informations. To achieve this same thing in Spark, it
takes more than a single line of code. Here we can see that our image is
$128\times 128$, with a 3rd dimension that corresponds to the RGB values since
we are working in colors.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%FLOWERS
    \subsection{Dask ML : flower classification}

    In this sections, we used Dask's framework to predict the class of flowers,
just as we did with Spark in the previous part. Dask was really easier to
approach, since doing machine learning with Dask is just doing the ML you know
(sklearn, xgboost, etc..) and plugging it to the dask cluster. The syntax stays
the same. For this part, all three models have the same predict score at the end
(93 \%), this is probably because of the size of the dataset, which is too small
to be able to see difference. The code is available in the python file.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%PARALLELISATION
    \subsection{Dask : parallelisation of the image processing algorithm 
    "MedianFilter"}
    
    In order to compute the parallelized median filter, we first have to
truncate ou picture in the number of partition that we want. Dask doesn't work
exactly like Spark. How I did is I first splited the Lena picture in the number
of partition.

    \begin{figure}[H]
          \centering
          \caption{Tiled Lena}
        \includegraphics[width=0.3\textwidth]{fig/lena_distributed.png}
    \end{figure}

Then, I distributed the calculus between all these separated images, wich are
all in different files. Here we have 8 partitions and our image is $128\times
128$, so each partition will be of size $128\times 16$, times 3 for the 3 RGB
canals. This means that we won't have to compute the local buffer
\textbf{inside} the function, as we did with Spark.

    \begin{figure}[H]
        \centering
        \caption{DAG}
    \includegraphics[width=0.5\textwidth]{fig/dask_graph_filter.png}
    \end{figure}

    The figure above shows part of the task graph that Dask has generated. Once
again, there was an
\href{}{implementation of the median filter}
function in the framework. The really difficult part here was to embrace the
framework and get use to Dasks DataFrames and parallelisation methods. But once
I understood the concepts, I found the framework much more intuitive and
user-friendly than Spark. In just a few lines of code, we could achieve the same
results as Spark.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CONCLUSION
\section*{Conclusions}

In this project, I tried to minimize \textbf{my} code. By this, I mean that
when I could find a useful function that does what I need (e.g. the median
filter), I decided to spend more time on reading the documentation and
integrating a new framework to my code, rather than trying to reinvent the wheel
by myself. This was intended, to focus more on learning how to learn new
framrworks than learning a particular one. Hadoop, Spark, and Dask give a level
of abstraction that makes one more productive while working on large data sets.
We do not have to manage the distribution of the files between the nodes, nor
the distribution of the calculus between the CPUs. But these tools still let us
adapt as much as we want our system to be working on one special environment. 

Doing the work of setting up the environment, installing the right modules or
setting the right system variables made me realize how much every technologies
are connected together, like Java, Python and C/C++ for this project. But it
also made me realize the hard work of developpers who make a high abstraction
level so people can work without having to think of hardware details as we have
to do when working in a C/C++ environment. Of course, even when using such
frameworks and high-level implementation, one must be aware of what happends
inside the blackbox, to better use it.

\bibliography{bibliography}
\end{document}
